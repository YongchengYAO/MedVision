tag: MedVision-BiometricsFromLandmarks,FeTA24
dataset_path: YongchengYAO/MedVision
dataset_kwargs:
  token: True
  trust_remote_code: True
test_split: test
fewshot_split: test
num_fewshot: 0
output_type: generate_until
doc_to_visual: !function utils.doc_to_visual
doc_to_text: !function utils.doc_to_text_BiometricsFromLandmarks
doc_to_target: !function utils.doc_to_target_BiometricsFromLandmarks
process_results: !function utils.process_results_BiometricsFromLandmarks
metric_list:
- metric: MAE
  aggregation: !function utils.aggregate_results_MAE
  higher_is_better: false
- metric: MRE
  aggregation: !function utils.aggregate_results_MRE
  higher_is_better: false
- metric: SuccessRate
  aggregation: !function utils.aggregate_results_SuccessRate
  higher_is_better: true

# Order: <model name>, default, dataset
# code: https://github.com/EvolvingLMMs-Lab/lmms-eval/blob/978eb7f1ccb994f580f9c74309321ad1cf2a5d7e/lmms_eval/api/task.py#L790
# NOTE: DO NOT change the keys in the model fields, as they are used in medvision_utils.py
lmms_eval_specific_kwargs:
  lingshu:
    model: "lingshu"
    sample_model_hf: "lingshu-medical-mllm/Lingshu-32B" 
  medgemma:
    model: "medgemma"
    sample_model_hf: "google/medgemma-4b-it"
  biomedgpt:
  healthgpt_l14:
    model: "healthgpt_l14"
    base_model_hf: "microsoft/phi-4"
    vision_model_hf: "openai/clip-vit-large-patch14-336"
    model_dtype: "FP16"
    hlora_r: 32
    hlora_alpha: 64
    hlora_dropout: 0
    hlora_nums: 4
    instruct_template: "phi4_instruct"
  llava_med:
    model: "llava_med"
    sample_model_hf: "microsoft/llava-med-v1.5-mistral-7b"
  meddr:
    model: "meddr"
    sample_model_hf: "Sunanhe/MedDr_0401"
  huatuogpt_vision:
    model: "huatuogpt_vision"
    sample_model_hf: "FreedomIntelligence/HuatuoGPT-Vision-34B"
  gemini__2_5:
  gemini__2_5_woTool:
  internvl3:
  llava_onevision:
  qwen2_5_vl:
    model: "qwen2_5_vl"
    sample_model_hf: "Qwen/Qwen2.5-VL-32B-Instruct"
  vllm_gemma3:
    model: "gemma3"
    sample_model_hf: "google/gemma-3-27b-it"
  vllm_internvl3:
    model : "internvl3"
    sample_model_hf: "OpenGVLab/InternVL3-38B"
  vllm_llama_3_2_vision:
    model: "llama_3_2_vision"
    sample_model_hf: "meta-llama/Llama-3.2-11B-Vision-Instruct"
  vllm_llava_onevision:
    model: "llava_onevision"
    sample_model_hf: "llava-hf/llava-onevision-qwen2-72b-ov-hf"
  vllm_qwen25vl:
    model: "qwen2_5_vl"
    sample_model_hf: "Qwen/Qwen2.5-VL-32B-Instruct"
  default:
    pre_prompt: ""
    post_prompt: ""
  dataset:

metadata:
- version: 1.0.0
